{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import spdiags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation\n",
    "\n",
    "So at this point, we now want to look at the following problem.  Suppose I give you some data in the form of a set of points\n",
    "\n",
    "$$\n",
    "\\left\\{x_{j},f_{j} \\right\\}_{j=0}^{n}\n",
    "$$\n",
    "\n",
    "where we think that $f_{j} = f(x_{j})$, which is to say, we think the data comes from a function $f(x)$, but we do not know the function $f(x)$.  Note, each point $x_{j}$ is called a _node_.  The question becomes, how might we find an approximation to $f(x)$?  As it turns out, there are an infinite number of ways to solve this problem, each with good and bad features.  The approach we will study first is called _ Lagrange Interpolation _.  \n",
    "\n",
    "This method starts by deciding we are going to fit the data with an $n^{th}$ order polynomial, i.e. we choose a polynomial $P_{n}(x)$ of the form \n",
    "\n",
    "$$\n",
    "P_{n}(x) = p_{0} + p_{1}x + \\cdots + p_{n}x^{n},\n",
    "$$\n",
    "\n",
    "where the coefficients $p_{j}$ are found from the _ interpolation formulas _\n",
    "\n",
    "$$\n",
    "P_{n}(x_{j}) = f_{j}, ~ j=0,\\cdots,n.\n",
    "$$\n",
    "\n",
    "As you can see, we have $n+1$ unknown coefficients $p_{j}$ and we have $n+1$ equations provided by the interpolation formulas.  Thus, we see that in principle we should be able to determine $P_{n}(x)$.  The advantage of having $P_{n}(x)$ is that anything else we want to know about $f(x)$, such as $f'(x)$ or $\\int f(x)dx$, we can find by using $P_{n}(x)$.  \n",
    "\n",
    "Now, there is another important way to think about $P_{n}(x)$.  While it is _ completely equivalent _ to what we have described above, it forms a very fundamental way to think about interpolation that comes up again and again in applied mathematics and numerical analysis.  In this approach, we write $P_{n}(x)$ as \n",
    "\n",
    "$$\n",
    "P_{n}(x) = \\sum_{j=0}^{n} f_{j}L_{j}^{(n)}(x),\n",
    "$$\n",
    "\n",
    "where the functions $L_{j}^{(n)}(x)$ are themselves $n^{th}$-order polynomials which are defined so that \n",
    "\n",
    "$$\n",
    "L_{j}^{(n)}(x_{j}) = 1, ~ L_{j}^{(n)}(x_{k}) = 0, ~k\\neq j. \n",
    "$$\n",
    "\n",
    "We can see this idea illustrated in the figure below.  Here, we are interpolating through the data set\n",
    "\n",
    "$$\n",
    "\\begin{array}{r|r}\n",
    "x_{j} & f_{j}\\\\\n",
    "\\hline\n",
    "-9 & 5\\\\\n",
    "-4 & 2\\\\\n",
    "-1 & -2\\\\\n",
    "7 & 9\n",
    "\\end{array}\n",
    "$$\n",
    "![linterp](https://upload.wikimedia.org/wikipedia/commons/5/5a/Lagrange_polynomial.svg)\n",
    "\n",
    "So,if we think about it, we ultimately see that we can find the $L^{(n)}_{j}(x)$ via the formula\n",
    "\n",
    "$$\n",
    "L_{j}^{(n)}(x) = \\frac{\\prod_{l\\neq j}^{n}(x-x_{l})}{\\prod_{l\\neq j}^{n}(x_{j}-x_{l})}.\n",
    "$$\n",
    "\n",
    "We can motivate this result by looking at simple cases and working our way up.  So, suppose we let $n=1$.  In this case, we need to find two linear functions $L^{(1)}_{0}(x)$ and $L^{(1)}_{1}(x)$.  As we can readily see, in order to satisfy our requirements for these functions, we can just use\n",
    "\n",
    "$$\n",
    "L^{(1)}_{0}(x) = \\frac{x-x_{1}}{x_{0}-x_{1}}, ~ L^{(1)}_{1}(x) = \\frac{x-x_{0}}{x_{1}-x_{0}}.\n",
    "$$\n",
    "\n",
    "We clearly see in this case that \n",
    "$$\n",
    "L^{(1)}_{0}(x_{0}) = 1, ~ L^{(1)}_{0}(x_{1}) = 0, ~~L^{(1)}_{1}(x_{0}) = 0, ~ L^{(1)}_{1}(x_{1}) = 1.\n",
    "$$\n",
    "\n",
    "Likewise, if we go to $n=2$, we need three different quadratic functions which, using either the formula or our intuition, we realize are \n",
    "\n",
    "$$\n",
    "L^{(2)}_{0}(x) = \\frac{(x-x_{1})(x-x_{2})}{(x_{0}-x_{1})(x_{0}-x_{2})}, ~ L^{(2)}_{1}(x) = \\frac{(x-x_{0})(x-x_{2})}{(x_{1}-x_{0})(x_{1}-x_{2})}, ~ L^{(2)}_{2}(x) = \\frac{(x-x_{0})(x-x_{1})}{(x_{2}-x_{1})(x_{2}-x_{1})}.\n",
    "$$\n",
    "\n",
    "So as we see, we can build $P_{n}(x)$ from the weighted $L^{(n)}_{j}(x)$ functions, which act as a _ basis _ for our interpolating polynomial.  The question then is, how can we numerically determine the functions $L^{(n)}_{j}(x)$.  This is done in the code below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lfun(xvals,jj,x):\n",
    "    lval = np.ones(x.size)\n",
    "    # Insert code here\n",
    "    return lval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lagran_interp(xvals,fvals,x):\n",
    "    n = fvals.size\n",
    "    # Insert code here \n",
    "    return ipoly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to recreate the example seen in the figure above, we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.array([-9.,-4.,-1.,7.])\n",
    "fvals = np.array([5.,2.,-2.,9.])\n",
    "xinterp = np.linspace(-9.,7.,int(1e2))\n",
    "ivals = lagran_interp(xvals,fvals,xinterp)\n",
    "plt.plot(xinterp,ivals,color='k')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$P_{3}(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we see that we have recreated said example exactly.  Okay, good, we have working code.  However, we are now going to look at a classic example of what is called the _Runge Phenomena_, which in effect shows us that we cannot really go all that far with Lagrange interpolation if we insist on using equally spaced points. \n",
    "\n",
    "To see this, we let \n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1+x^{2}}, ~ -1\\leq x \\leq 1.\n",
    "$$\n",
    "\n",
    "Now keep in mind, the more nodes I use in my interpolation scheme, the higher the degree polynomial I am obliged to use.  So suppose I use relatively few nodes, like only 10.  Then I get the following error plot for Lagrange interpolation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xinterp = np.linspace(-1.,1.,int(1e3))\n",
    "ftrue = 1./(1.+xinterp**2.)\n",
    "xvals = xinterp[::100]\n",
    "fvals = ftrue[::100]\n",
    "finterp = lagran_interp(xvals,fvals,xinterp)\n",
    "plt.plot(xinterp,np.log10(np.abs(ftrue-finterp)),ls='-',color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we double the number of points, aside from the fact that the error is several orders of magnitude larger at the right end point, overall, everything improves.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = \n",
    "fvals = \n",
    "finterp = lagran_interp(xvals,fvals,xinterp)\n",
    "plt.plot(xinterp,np.log10(np.abs(ftrue-finterp)),ls='-',color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, at this point, we might imagine that more nodes means greater accuracy.  And that is in some sense true.  And if we double the number of points again, we get "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = \n",
    "fvals = \n",
    "finterp = lagran_interp(xvals,fvals,xinterp)\n",
    "plt.plot(xinterp,np.log10(np.abs(ftrue-finterp)),ls='-',color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So again, endpoints are problematic, but overall everything improved.  So if we keep adding nodes, then we should eventually be able to get to machine precision, right? Wrong.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = \n",
    "fvals = \n",
    "finterp = lagran_interp(xvals,fvals,xinterp)\n",
    "plt.plot(xinterp,np.log10(np.abs(ftrue-finterp)),ls='-',color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, more nodes actually made our approxmiation _worse_, not better.  Further, as we see, while we can achieve machine precision on the interior of our interpolation scheme, we cannot get anywhere close to that at the boundaries.  In order to get around this then, we might think of a different way to position nodes to address this issue.  Thus, we get motivate discussing clustered meshes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Meshes\n",
    "\n",
    "So, a way to get around the Runge Phenomena is to use unevenly spaced meshes of points.  To wit, we use what are called the Chebyshev points or nodes, which are given by \n",
    "\n",
    "$$\n",
    "x_{j} = \\cos\\left(\\frac{2j+1}{2n+2}\\pi\\right), ~ j=0,\\cdots,n\n",
    "$$\n",
    "\n",
    "As we see below, by essentially clustering nodes at the endpoints of the interval we wish to interpolate over, we can remove the Runge Phenomena.  This incidentally is the beginning of a long conversation in numerical analysis we will not pursue further here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncheb = 40\n",
    "xcheb = \n",
    "fcheb = \n",
    "finterp = lagran_interp(xcheb,fcheb,xinterp)\n",
    "plt.plot(xinterp,np.log10(np.abs(ftrue-finterp)),ls='-',color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Splines\n",
    "\n",
    "So, an alternative to our approach above is to use a more less global, or more global approach, in which we do interpolation via pieces wise defined functions called _ splines _.  As before, we start our discussion with a data set \n",
    "\n",
    "$$\n",
    "\\left\\{x_{j},f_{j}\\right\\}_{j=0}^{n}.\n",
    "$$\n",
    "\n",
    "We now define our _cubic splines_ $S_{j}(x)$ to be third order polynomials, i.e. \n",
    "\n",
    "$$\n",
    "S_{j}(x) = a_{j}(x-x_{j})^{3} + b_{j}(x-x_{j})^{2} + c_{j}(x-x_{j}) + d_{j},\n",
    "$$\n",
    "\n",
    "such that \n",
    "\n",
    "\\begin{align}\n",
    "S_{j}(x_{j}) = & f_{j}, ~ j=0,\\cdots,n-1\\\\\n",
    "S_{j}(x_{j+1}) = & S_{j+1}(x_{j+1}), ~ j=0,\\cdots,n-2 \\\\\n",
    "S'_{j}(x_{j+1}) = & S'_{j+1}(x_{j+1}), ~ j=0,\\cdots,n-2 \\\\\n",
    "S''_{j}(x_{j+1}) = & S''_{j+1}(x_{j+1}), ~ j=0,\\cdots,n-2 \n",
    "\\end{align}\n",
    "\n",
    "which is to say, we require that we interpolate the data, and the each spline as as its first and second derivatives be continuous at each node.  Finally, we require that \n",
    "\n",
    "$$\n",
    "S_{n-1}(x_{n}) = f_{n}, ~ S''_{0}(x_{0})=0, ~ S''_{n-1}(x_{n}) = 0.\n",
    "$$\n",
    "\n",
    "We readily see then that $d_{j}= f_{j}$.  Define \n",
    "\n",
    "$$\n",
    "\\delta x_{j} = x_{j+1} -x_{j}, ~ \\delta f_{j} = f_{j+1} - f_{j}.\n",
    "$$\n",
    "\n",
    "Then from above we get the system of equations for  $j=0,\\cdots,n-2$\n",
    "\n",
    "\\begin{align}\n",
    "a_{j}(\\delta x_{j})^{2} + b_{j}\\delta x_{j} + c_{j} = & \\frac{\\delta f_{j}}{\\delta x_{j}},\\\\\n",
    "3a_{j}(\\delta x_{j})^{2} + 2b_{j}\\delta x_{j} + c_{j} = & c_{j+1},\\\\\n",
    "3a_{j}\\delta x_{j} + b_{j} = & b_{j+1} \n",
    "\\end{align}\n",
    "\n",
    "The end point conditions give us\n",
    "\n",
    "$$\n",
    "a_{n-1}\\left(\\delta x_{n-1}\\right)^{2} + b_{n-1}\\delta x_{n-1} + c_{n-1} = \\frac{\\delta f_{n-1}}{\\delta x_{n-1}},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "b_{0}=0, ~ 3a_{n-1}\\delta x_{n-1} + b_{n-1} = 0.\n",
    "$$\n",
    "\n",
    "Solving for $a_{j}$ gives us, \n",
    "\n",
    "$$\n",
    "a_{j} = \\frac{1}{\\delta x_{j}^{2}}\\left(\\frac{\\delta f_{j}}{\\delta x_{j}} - c_{j} - b_{j}\\delta x_{j}\\right), ~ j=0,\\cdots,n-1,\n",
    "$$\n",
    "\n",
    "and in turn we then find that \n",
    "\n",
    "\\begin{align}\n",
    "3\\frac{\\delta f_{j}}{\\delta x_{j}} - b_{j}\\delta x_{j} - 2c_{j} = & c_{j+1}, ~ j=0,\\cdots,n-2\\\\\n",
    "3\\frac{\\delta f_{j}}{(\\delta x_{j})^{2}} - 3\\frac{c_{j}}{\\delta x_{j}} - 2b_{j} = & b_{j+1}, ~j=0,\\cdots,n-2,\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "c_{n-1} = \\frac{\\delta f_{n-1}}{\\delta x_{n-1}} - \\frac{2}{3}\\delta x_{n-1}b_{n-1}.\n",
    "$$\n",
    "\n",
    "Likewise, solving for $c_{j}$ then gives us\n",
    "\n",
    "$$\n",
    "c_{j} = \\frac{\\delta f_{j}}{\\delta x_{j}} - \\frac{\\delta x_{j}}{3}\\left( 2b_{j} + b_{j+1}\\right), ~ j=0,\\cdots,n-2.\n",
    "$$\n",
    "\n",
    "Ultimately then, we arrive at the system of equations, for $j=1,\\cdots,n-3$, \n",
    "\n",
    "$$\n",
    "\\frac{\\delta x_{j}}{3} b_{j} + \\frac{2}{3}\\left(\\delta x_{j} + \\delta x_{j+1} \\right)b_{j+1} + \\frac{\\delta x_{j+1}}{3} b_{j+2} = \\frac{\\delta f_{j+1}}{\\delta x_{j+1}} - \\frac{\\delta f_{j}}{\\delta x_{j}},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{2}{3}\\left(\\delta x_{0} + \\delta x_{1} \\right)b_{1} + \\frac{\\delta x_{1}}{3} b_{2} = \\frac{\\delta f_{1}}{\\delta x_{1}} - \\frac{\\delta f_{0}}{\\delta x_{0}},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\delta x_{n-2}}{3} b_{n-2} + \\frac{2}{3}\\left(\\delta x_{n-2} + \\delta x_{n-1} \\right)b_{n-1} = \\frac{\\delta f_{n-1}}{\\delta x_{n-1}} - \\frac{\\delta f_{n-2}}{\\delta x_{n-2}}.\n",
    "$$\n",
    "\n",
    "At this point, we should talk about solving the problem $Ab=\\delta f$ where $A$ is a self-adjoint tridiagonal matrix, but that is beyond the scope of this course.  Suffice to say that NumPy makes doing this straightforward and efficient.  This is done in the code below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spline_maker(xvals,fvals,qvals):\n",
    "    # m = fvals.size\n",
    "    # note, from above, n = m-1\n",
    "    \n",
    "    n = fvals.size - 1\n",
    "    df = fvals[1:]-fvals[0:n]\n",
    "    dx = xvals[1:]-xvals[0:n]\n",
    "    dfdx = df/dx\n",
    "    svals = np.zeros(ivals.size)\n",
    "    \n",
    "    rhs = dfdx[1:] - dfdx[0:n-1]\n",
    "    diag = 2./3.*(dx[1:] + dx[0:n-1])\n",
    "    data = np.array([diag,dx[1:]/3.,dx[0:n-1]/3.])\n",
    "    dvals = np.array([0,-1,1])\n",
    "    Bmat = spdiags(data, dvals, n-1, n-1)\n",
    "    bvec = spsolve(Bmat,rhs)\n",
    "    bvec = np.append(0.,bvec)\n",
    "    \n",
    "    cvec = dfdx - 2./3.*dx*bvec - dx/3.*np.append(bvec[1:],0.)\n",
    "    avec = (dfdx - dx*bvec - cvec)/(dx**2.)\n",
    "    \n",
    "    for jj in xrange(1,n+1):\n",
    "        \n",
    "        indsr = qvals < xvals[jj] \n",
    "        indsl = qvals >= xvals[jj-1]\n",
    "        inds = indsl*indsr\n",
    "        \n",
    "        dxloc = qvals[inds] - xvals[jj-1]\n",
    "        svals[inds] = avec[jj-1]*dxloc**3. + bvec[jj-1]*dxloc**2. + cvec[jj-1]*dxloc + fvals[jj-1]\n",
    "        \n",
    "    return svals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisiting the example from above in which \n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1+x^{2}}, ~ -1\\leq x \\leq 1,\n",
    "$$\n",
    "\n",
    "we can now test our spline approximation scheme.  As we show, it can be very accurate, and while our arbitrary choice of enforcing zero curvature at the endpoints does cost us some amount of accuracy, we do not have anything resembling the problems we saw above with Lagrange interpolation.  Thus, splines offer us an accurate, efficient, and flexible means of interpolating data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvs = np.linspace(-1.,1.,int(1e3)+1)\n",
    "fvs = 1./(1.+xvs**2.)\n",
    "ivals = np.linspace(-.99,.99,int(5e3))\n",
    "ftrue = 1./(1.+ivals**2.)\n",
    "\n",
    "svals = spline_maker(xvs,fvs,ivals)\n",
    "#plt.plot(ivals,svals,ls='-',color='k')\n",
    "#plt.plot(ivals,ftrue,ls='--',color='r')\n",
    "plt.plot(ivals,np.log10(np.abs(ftrue-svals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
