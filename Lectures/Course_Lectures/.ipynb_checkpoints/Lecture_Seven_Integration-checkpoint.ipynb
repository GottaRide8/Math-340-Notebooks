{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Quadrature\n",
    "\n",
    "Okay, so now we are going to get into one of the biggest reasons we do math on computers, and that is to numerically compute integrals.  What we are attempting to do then is, for a given function $f(x)$, find approximations to \n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) = \\int_{a}^{b}f(x) dx.\n",
    "$$\n",
    "\n",
    "To do this, we first need to introduce the notion of a mesh $\\left\\{x_{i}\\right\\}_{j=0}^{N}$ where\n",
    "\n",
    "$$\n",
    "x_{0} =a, ~ x_{N} = b, ~ x_{i+1}-x_{i} = \\delta x = \\frac{b-a}{N}.\n",
    "$$\n",
    "\n",
    "## The Trapezoid Method\n",
    "\n",
    "Thus, our first attempt at developing an approximation scheme will be to find approximations over intervals $[x_{i},x_{i+1}]$.  Then we will use the identity\n",
    "\n",
    "$$\n",
    "\\int_{a}^{b} f(x) dx = \\sum_{i=0}^{N-1}\\int_{x_{i}}^{x_{i+1}}f(x)dx.\n",
    "$$\n",
    "\n",
    "Now, on each interval $[x_{i},x_{i+1}]$, we now replace $f(x)$ with a straight-line approximation which connects the points $(x_{i},f(x_{i}))$ and $(x_{i+1},f(x_{i+1}))$.\n",
    "\n",
    "![trap](https://upload.wikimedia.org/wikipedia/commons/d/d1/Integration_num_trapezes_notation.svg)\n",
    "\n",
    "What we are saying here is that we make the assumption that\n",
    "\n",
    "$$\n",
    "f(x) \\approx f(x_{i}) + \\frac{(f(x_{i+1})-f(x_{i}))}{\\delta x}(x-x_{i}), ~ x\\in[x_{i},x_{i+1}].\n",
    "$$\n",
    "\n",
    "The advantage of doing this is that we can readily compute the integral of the right-hand side of our approximation.\n",
    "\n",
    "\n",
    "_ Problem _: Using the approximation to the function, find an approximation to the integral of $f(x)$ over the interval $[x_{i},x_{i+1}]$.  \n",
    "\n",
    "So as we discussed in class, this is given by \n",
    "\n",
    "$$\n",
    "\\int_{x_{j}}^{x_{j+1}} f(x) dx \\approx \\frac{\\delta x}{2}\\left(f(x_{j}) + f(x_{j+1})\\right)\n",
    "$$\n",
    "\n",
    "_ Problem _: Combine your approximations over each sub interval $[x_{i},x_{i+1}]$ to build an approximation for the integral of $f(x)$ on $[a,b]$.    \n",
    "\n",
    "Again, as discussed in class, this becomes \n",
    "\n",
    "$$\n",
    "\\int_{a}^{b}f(x) dx \\approx \\frac{\\delta x}{2}\\left(f(a) + f(b) + 2\\sum_{j=1}^{N-1}f(x_{j})\\right)\n",
    "$$\n",
    "\n",
    "_ Problem _: Code it.  Test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trapezoid_method(a,b,N,f):\n",
    "    Nind = int(N)\n",
    "    fvals = f(np.linspace(a,b,Nind+1))\n",
    "    return (b-a)/(2*N)*(fvals[0]+fvals[Nind]+2.*np.sum(fvals[1:Nind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.459693863311\n",
      "0.459697694132\n"
     ]
    }
   ],
   "source": [
    "def f(xvals):\n",
    "    fvals = np.sin(xvals)\n",
    "    return fvals\n",
    "print trapezoid_method(0.,1.,100.,f)\n",
    "print 1. - np.cos(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis for the Trapezoid Method\n",
    "\n",
    "If we denote the approximation to $T_{[a,b]}(f)$ via the Trapezoid Method as $A_{N}(f)$, we want to figure out how the difference between these two things behaves as a function of $N$, or equivalently, $\\delta x$.  We suppose that \n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) = A_{N}(f) + C\\left(\\delta x\\right)^{p}, ~ \\delta x = \\frac{b-a}{N}.\n",
    "$$\n",
    "\n",
    "So how can we find $p$?  Well, our friend the logarithm returns since we see that \n",
    "\n",
    "$$\n",
    "\\log_{10}\\left|T_{[a,b]}(f) - A_{N}(f) \\right| = \\log_{10}C + p\\left(\\log_{10}(b-a) - \\log_{10} N \\right)\n",
    "$$\n",
    "\n",
    "Thus, if we compare the log of the error to the log of the number of intervals in our mesh, then the slope of that line should be the rate of decay, or how the error decreases as we increase the number of points in the mesh. \n",
    "\n",
    "_ Problem _: Write code to find $p$.  This means you need to think of a test case for which you know the answer and then generate a series of comparisons which allow you to infer a trend.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_test(a,b,f):\n",
    "    tval = 0.\n",
    "    Nvals = np.array([1e1, 1e2, 1e3, 1e4, 1e5, 1e6])\n",
    "    Evals = np.zeros(Nvals.size)\n",
    "    for jj in xrange(0,Nvals.size):\n",
    "        Evals[jj] = np.log10(np.abs(tval - trapezoid_method(a,b,Nvals[jj],f)))\n",
    "    xvals = np.log10(Nvals)\n",
    "    plt.plot(xvals,Evals)\n",
    "    novals = Evals.size\n",
    "    slopes = (Evals[1:]-Evals[0:novals-1])/(xvals[1:]-xvals[0:novals-1])\n",
    "    print np.min(slopes)\n",
    "    print np.max(slopes)\n",
    "    print np.mean(slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.09500866611\n",
      "1.9672312559\n",
      "0.218697927048\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8477a51f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_test(0.,2.*np.pi,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Simpon's Method\n",
    "\n",
    "So, if lines were great, what if we used a more accurate means of approximating a function?  To do this, we will work over three mesh points, say $[x_{j},x_{j+2}]$, and then make the approximation\n",
    "\n",
    "$$\n",
    "f(x) \\approx a_{j+1}(x-x_{j+1})^{2} + b_{j+1}(x-x_{j+1}) + c_{j+1},\n",
    "$$\n",
    "\n",
    "with the interpolation requirements\n",
    "\n",
    "\\begin{align}\n",
    "y_{j+1}(x_{j}) = & f(x_{j})\\\\\n",
    "y_{j+1}(x_{j+1}) = & f(x_{j+1})\\\\\n",
    "y_{j+1}(x_{j+2}) = & f(x_{j+2})\n",
    "\\end{align}\n",
    "\n",
    "![simp](https://jeremykun.files.wordpress.com/2011/12/simpson.png?w=1800)\n",
    "\n",
    "Then, using this approximation, we approximate the integral of $f$ over $[x_{j},x_{j+2}]$ via \n",
    "\n",
    "$$\n",
    "\\int_{x_{j}}^{x_{j+2}}f(x) dx \\approx \\int_{x_{j}}^{x_{j+2}} y_{j+1}(x)dx.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adaptive Quadrature\n",
    "\n",
    "So, as we you show in the homework, letting\n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) = \\int_{a}^{b}f(x)dx\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "A_{[a,b]}(f) = \\frac{(b-a)}{6}\\left(f(a) + 4f(c) + f(b)\\right), ~ c = \\frac{a+b}{2},\n",
    "$$\n",
    "\n",
    "Simpson's method gives us\n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) \\approx A_{[a,b]}(f) + C_{0}(b-a)^{5}.\n",
    "$$\n",
    "\n",
    "Note, the true story is actually a bit more complicated, and we really should write\n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) = A_{[a,b]}(f) + C_{0}(b-a)^{5} + C_{1}(b-a)^{6} + \\cdots.\n",
    "$$\n",
    "\n",
    "In this vein, if we split $T_{[a,b]}(f)$ into \n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) = T_{[a,c]}(f) + T_{[c,b]}(f), ~ c = \\frac{a+b}{2},\n",
    "$$\n",
    "\n",
    "then we we can build a better approximation by using\n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) = A_{[a,c]}(f) + A_{[c,b]}(f) + \\frac{C_{0}}{16}(b-a)^{5} + \\cdots\n",
    "$$\n",
    "\n",
    "Now, where this gets really tricky is that we can do better yet still.  Here is how.  From my new approximation, I can write\n",
    "\n",
    "$$\n",
    "16 T_{[a,b]}(f) = 16\\left(A_{[a,c]}(f) + A_{[c,b]}(f)\\right) + C_{0}(b-a)^{5} + \\cdots\n",
    "$$\n",
    "\n",
    "Using the old approximation, I get \n",
    "\n",
    "$$\n",
    "15 T_{[a,b]}(f) = 16\\left(A_{[a,c]}(f) + A_{[c,b]}(f)\\right) - A_{[a,b]}(f) + \\cdots\n",
    "$$\n",
    "\n",
    "and thus we get the new and improved approximation\n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) = \\frac{16}{15}\\left(A_{[a,c]}(f) + A_{[c,b]}(f)\\right) - \\frac{1}{15}A_{[a,b]}(f) + \\cdots\n",
    "$$\n",
    "\n",
    "So now think about that.  By building two different approximations, I am able to get a third, yet better one.  But we can also ask another kind of question.  What if \n",
    "\n",
    "$$\n",
    "\\left|\\left(A_{[a,c]}(f) + A_{[c,b]}(f)\\right) - A_{[a,b]}(f)\\right| < \\mbox{tol} ~ \\mbox{?}\n",
    "$$\n",
    "\n",
    "As in, okay, I make two approximations, one okay, one better.  But the difference between them is not so large?  Then what is the point of continuing to make smaller subdivisions of the interval when I am actually happy with what I have got?  So, how would I implement a method based on this line of thinking?  And what would it get me?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adap_quad_comp():\n",
    "    \n",
    "    d = (a+c)/2.\n",
    "    e = (c+b)/2.\n",
    "    fd = f(d)\n",
    "    fe = f(e)\n",
    "    A2 = dx/12.*(fa + 4.*fd + 2.*fc + 4.*fe + fb)\n",
    "    \n",
    "    if np.abs(A2-A)<tol:\n",
    "        return 16./15.*A2 - A/15.\n",
    "    else:\n",
    "        return adap_quad_comp() + adap_quad_comp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adap_quad(f,a,b,tol):\n",
    "    c = (a+b)/2.\n",
    "    fa = f(a)\n",
    "    fb = f(b)\n",
    "    fc = f(c)\n",
    "    dx = b-a\n",
    "    A = dx/6.*(fa + 4.*fc + fb)\n",
    "    \n",
    "    adap_quad_comp()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45969769039\n",
      "0.459697694132\n"
     ]
    }
   ],
   "source": [
    "faq = lambda x : np.sin(x)\n",
    "print adap_quad(faq,0.,1.,1e-4)\n",
    "print 1. - np.cos(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
